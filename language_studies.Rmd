---
title: "Language_Analysis"
output:
  word_document: default
  html_document: default
  pdf_document: default
date: "2022-09-08"
---

```{r, echo=FALSE, include=FALSE, warning=FALSE}
# Libraries Needed
library(reshape2)
library(hrbrthemes)
library(scales)
library(ggsci)
library(data.table)
library(dplyr)
library(ggplot2)
library(readr)
library(stringr)
library(lubridate)
library(lme4)
library(nlme)
library(stargazer)
library(caret)
library(rcompanion)
library(tm)
library(performance)
library(tidytext)
library(gridExtra)
library(trend)

#Functions
logit2prob <- function(logit){
  odds <- exp(logit)
  prob <- odds / (1 + odds)
  return(prob)
}

stargazer2 <- function(model, odd.ratio = F, ...) {
  if(!("list" %in% class(model))) model <- list(model)
    
  if (odd.ratio) {
    coefOR2 <- lapply(model, function(x) exp(coef(x)))
    seOR2 <- lapply(model, function(x) exp(coef(x)) * summary(x)$coef[, 2])
    p2 <- lapply(model, function(x) summary(x)$coefficients[, 4])
    stargazer(model, coef = coefOR2, se = seOR2, p = p2, ...)
    
  } else {
    stargazer(model, ...)
  }
}
launch_date <- as.POSIXct("2016-10-12 12:00:00",  format="%Y-%m-%d %H:%M:%S")

# Import words
domain <- read.table("/Volumes/cbjackson2/language models/domain_clean.txt", quote="\"", comment.char="")
domain <- c(domain$x)

field <- read.table("/Volumes/cbjackson2/language models/science_clean.txt", quote="\"", comment.char="")
field <- c(field$x)
```

# Project Growth 
```{r project-growth, ech0=FALSE}

# How the project grows over time
all_comments <- read_csv("/Volumes/cbjackson2/language models/comments_newcomer_add.csv") # All project comments
all_comments <- all_comments[,-c(1:4)]

# Add summaries
all_comments$tags <- stringr::str_count(all_comments$comment_body, "\\#") 
all_comments$user_references <- stringr::str_count(all_comments$comment_body, "\\@") 
all_comments$questions <- stringr::str_count(all_comments$comment_body, "\\?") 
all_comments$domain_words <- stringr::str_count(all_comments$comment_body, paste(domain, collapse='|'))
all_comments$field_words <- stringr::str_count(all_comments$comment_body, paste(field, collapse='|'))
 
# Compute power laws https://cran.r-project.org/web/packages/poweRlaw/vignettes/b_powerlaw_examples.pdf and https://towardsdatascience.com/analysing-power-law-distributions-with-r-4312c7b4261b 
############################################################
############################################################
############################################################
# COMMENTS 

all_comments_summary <- all_comments  %>%  
  group_by(week=floor_date(comment_created_at, "week")) %>% 
  summarize(comment_count =length(comment_id)) %>%
  mutate(
         comment_growth = cumsum(comment_count),
         comment_pct = cumsum(comment_count)/sum(comment_count),
         comment_diff_growth = comment_pct - lag(comment_pct))

############################################################
############################################################
############################################################
# WORDS 

all_words <- data.frame(read_csv("/Volumes/cbjackson2/language models/worddata/word_first_use.csv")) # A


#word_use <- read_csv("/Volumes/cbjackson2/language models/worddata/word_use_clean.csv") 

all_words_summary <- all_words  %>%  
  group_by(week=floor_date(firstuse, "week")) %>% 
  summarize(word_count =length(unigram)) %>%
  mutate(
         word_growth = cumsum(word_count),
         word_pct = cumsum(word_count)/sum(word_count),
         word_diff_growth = word_pct - lag(word_pct))

### Get growth in field and domain words
all_words$type <- ifelse(all_words$unigram %in% field,"Field",
                           ifelse(all_words$unigram %in% domain,"Domain","Regular"))


# A container is needed to compute since some weeks might not have new domain or field words
# Get all weeks 

weeks_s <- data.frame(all_comments_summary$week)
names(weeks_s)[1] <- "week"

domain_words_summary <- all_words  %>%  
  filter(type=="Domain") %>% 
  group_by(week=floor_date(firstuse, "week")) %>% 
  summarize(domain_count =length(unigram))

domain_words_summary <- merge(weeks_s,domain_words_summary, by = "week",all.x=TRUE)
domain_words_summary$domain_count[is.na(domain_words_summary$domain_count)] <- 0  #fill NA with 0 

domain_words_summary <- domain_words_summary  %>%  
   mutate(
         domain_growth = cumsum(domain_count),
         domain_pct = cumsum(domain_count)/sum(domain_count),
         domain_diff_growth = domain_pct - lag(domain_pct))


field_words_summary <- all_words  %>%  
  filter(type=="Field") %>% 
  group_by(week=floor_date(firstuse, "week")) %>% 
  summarize(field_count =length(unigram))

field_words_summary <- merge(weeks_s,field_words_summary, by = "week",all.x=TRUE)
field_words_summary$field_count[is.na(field_words_summary$field_count)] <- 0  #fill NA with 0 

field_words_summary <- field_words_summary  %>%  
   mutate(
         field_growth = cumsum(field_count),
         field_pct = cumsum(field_count)/sum(field_count),
         field_diff_growth = field_pct - lag(field_pct))

remove(weeks_s)


############################################################
############################################################
############################################################
# UNIGRAMS 
unigrams <- read_csv("/Volumes/cbjackson2/language models/worddata/unigram_comments.csv") # All unigrams
unigrams <- data.frame(unigrams)

all_token_summary <- unigrams  %>%  
  group_by(week=floor_date(comment_created_at, "week")) %>% 
  summarize(token_count =length(unigram)) %>%
  mutate(
         token_growth = cumsum(token_count),
         token_pct = cumsum(token_count)/sum(token_count),
         token_diff_growth = token_pct - lag(token_pct))

############################################################
############################################################
############################################################
# THREADS 

all_threads_summary <- all_comments  %>%  
  group_by(week=floor_date(comment_created_at, "week")) %>% 
  summarize(thread_count =length(discussion_id)) %>%
  mutate(
         thread_growth = cumsum(thread_count),
         thread_pct = cumsum(thread_count)/sum(thread_count),
         thread_diff_growth = thread_pct - lag(thread_pct))

##### Merge data
eco_grow <- merge(all_comments_summary,all_words_summary, by="week", all.x = TRUE)
eco_grow <- merge(eco_grow,all_threads_summary, by="week", all.x = TRUE)
eco_grow <- merge(eco_grow,all_token_summary, by="week", all.x = TRUE)
eco_grow <- merge(eco_grow,domain_words_summary, by="week", all.x = TRUE)
eco_grow <- merge(eco_grow,field_words_summary, by="week", all.x = TRUE)

remove(all_comments_summary,all_words_summary,all_threads_summary,all_token_summary,domain_words_summary,field_words_summary)

####################  
####################  Visualize data (3x2) grid
####################  


### Visualizing aggregate growth
eco_grow_plot <- eco_grow[,c(1,3,7,11,15,19,23)] 
eco_grow_plot$week <- as.Date(eco_grow_plot$week)
eco_grow_plot.m <- melt(eco_grow_plot, id.vars="week")

eco_grow_plot.m$variable <-  recode(eco_grow_plot.m$variable, comment_growth = "Comments", word_growth = "New Tokens",thread_growth = "Threads", token_growth = "All Tokens",
                                    domain_growth = "Domain Tokens", field_growth = "Field Tokens")

feature_growth <- ggplot(eco_grow_plot.m, aes(week, value)) + 
  geom_line(size=1) + 
  geom_smooth(size=.7, color="grey") +
  labs(x="Date",y="Feature growth") + 
  scale_x_date(date_labels="%b %y",date_breaks  ="6 month") + 
  theme_classic() + scale_color_nejm() +
  #scale_y_continuous(labels = function(x) paste0(x*100, "%")) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1),
        legend.position="none", 
        legend.title = element_blank(),
        axis.text = element_text(size = 8),
        axis.title = element_text(size = 10, face="bold")) + 
  geom_vline(aes(xintercept=as.Date("2016-10-12")),size=.25) + 
  geom_text(aes(x=as.Date("2016-10-11"), label="", y=.50), colour="grey", angle=90, vjust = -1, text=element_text(size=2)) +
    facet_wrap(~variable, scales="free_y")


pdf("/Users/jackson/Library/CloudStorage/Box-Box/_research/language[collab]/feature_growth.pdf", width=11, height=4)
feature_growth
dev.off()

### Visualizing weekly contributions (counts)
eco_count_plot <- eco_grow[,c(1,2,6,10,14,18,22)] 
eco_count_plot$week <- as.Date(eco_count_plot$week)
eco_count_plot.m <- melt(eco_count_plot, id.vars="week")

eco_count_plot.m$variable <-  recode(eco_count_plot.m$variable, comment_count = "Comments", word_count = "New Tokens",thread_count = "Threads", token_count = "All Tokens",
                                    domain_count = "Domain Tokens", field_count = "Field Tokens")

feature_count <- ggplot(eco_count_plot.m, aes(week, value)) + 
  geom_point(size=.1) + 
  geom_smooth(size=.7, color="grey") +
  labs(x="Date",y="Feature growth") + 
  scale_x_date(date_labels="%b %y",date_breaks  ="6 month") + 
  theme_classic() + scale_color_nejm() +
  #scale_y_continuous(labels = function(x) paste0(x*100, "%")) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1),
        legend.position="none", 
        legend.title = element_blank(),
        axis.text = element_text(size = 8),
        axis.title = element_text(size = 10, face="bold")) + 
  geom_vline(aes(xintercept=as.Date("2016-10-12")),size=.25) + 
  geom_text(aes(x=as.Date("2016-10-11"), label="", y=.50), colour="grey", angle=90, vjust = -1, text=element_text(size=2)) +
    facet_wrap(~variable, scales="free_y")

pdf("/Users/jackson/Library/CloudStorage/Box-Box/_research/language[collab]/feature_count.pdf", width=11, height=4)
feature_count
dev.off()

####################  
####################  Stat Analysis
####################  

##### Conduct pettit test (changepoint) https://rpubs.com/shirazipooya/Pettitt-Test on the number of values each day 
growth_pettit <- eco_count_plot.m %>% 
  group_by(variable) %>% 
  mutate(statistics = pettitt.test(value)$statistic,
         estimate = pettitt.test(value)$estimate,
         p_value = pettitt.test(value)$p.value,
         week = eco_count_plot.m[['week']][pettitt.test(value)$estimate]) %>% 
  distinct(variable,statistics,estimate,week,p_value)

##### Conduct growth models (answering ... which growth model describes content) https://rpubs.com/jaelison/200149 or #https://www.marinedatascience.co/blog/2019/09/28/comparison-of-change-point-detection-methods/








```



```{r }

# For each newcomer (for modeling) can we include some measure of the community during each week? 

newcomers_data <- read_csv("/Volumes/cbjackson2/language models/newcomer_info.csv")
newcomers_data <- newcomers_data[,-1]
newcomers_week_data <-  read_csv("/Volumes/cbjackson2/language models/newcomer_week_info.csv") 
newcomers_week_data <- data.frame(newcomers_week_data)
newcomers_week_data$week <- as.Date(newcomers_week_data$week)
newcomers_week_data$user_id <- as.factor(newcomers_week_data$user_id)
comments <- read_csv("/Volumes/cbjackson2/language models/comments_newcomer_add.csv") 




# Add comment features
comments$tags <- stringr::str_count(comments$comment_body, "\\#") 
comments$user_references <- stringr::str_count(comments$comment_body, "\\@") 
comments$domain_words <- stringr::str_count(comments$comment_body, paste(domain, collapse='|'))
comments$field_words <- stringr::str_count(comments$comment_body, paste(field, collapse='|'))

# Add newcomer comment features to newcomers_data
comments <- merge(comments,newcomers_data[,c("user_id","join_date")],by.y="user_id",by.x = "comment_user_id",all.x = TRUE)

#comments$include <- ifelse(comments$comment_created_at <= add_with_rollback(comments$join_date, months(3), roll_to_first = TRUE),1,0)
#comments <- comments[which(comments$include==1),]

comment_summary <- comments %>% 
  group_by(user_id=comment_user_id) %>%
  summarise(comment_no=length(comment_id),
            questions = sum(questions),
            replies = sum(replies),
            links = sum(links),
            words = sum(words),
            tags = sum(tags),
            user_references = sum(user_references),
            domain_words = sum(domain_words),
            field_words = sum(field_words)
  )


comment_week_summary <- comments %>% 
  group_by(user_id=as.factor(comment_user_id), week=floor_date(created_at, "week")) %>%
  summarise(comment_no=length(comment_id),
            questions = sum(questions),
            replies = sum(replies),
            links = sum(links),
            words = sum(words),
            tags = sum(tags),
            user_references = sum(user_references),
            domain_words = sum(domain_words),
            field_words = sum(field_words)
  )

comment_week_summary <- data.frame(comment_week_summary)
comment_week_summary <- comment_week_summary[order(comment_week_summary$user_id,comment_week_summary$week),]
comment_week_summary <- as.data.table(comment_week_summary)[, comment_week := 1:.N,, by = list(user_id)] 
comment_week_summary <- data.frame(comment_week_summary)
 
newcomers_data <- merge(newcomers_data,comment_summary, by="user_id",all.x = TRUE)
newcomers_data$since_start <- round(as.numeric(difftime(newcomers_data$join_date,launch_date,units = "days")),digits = 0)
newcomers_data_org <- newcomers_data
remove(comment_summary)

#newcomers_data <- newcomers_data[which(newcomers_data$join_date >= as.Date(launch_date)),] # removed 1096 users who were around prior to launch

### Build Cosine dataset
domain_cosine <- read_csv("/Volumes/cbjackson2/language models/cosine_domain.csv") 
names(domain_cosine)[c(4,8)] <- c("domain_cosine","domain_change")
science_cosine <- read_csv("/Volumes/cbjackson2/language models/cosine_science.csv") 

cosine <- merge(domain_cosine,science_cosine, by=c("user_id","variable"))
cosine <- cosine[,c(1,2,4,8,5,6,7,10,14)]
names(cosine)[c(5:9)] <- c("real_week","comment_week","last_session","science_cosine","science_change")

# Get only newcomer cosine information
newcomers_period <-  newcomers_data[,c(1,9)]

cosine <- merge(cosine,newcomers_period,by="user_id")
cosine <- cosine[which(cosine$comment_week <= cosine$newcomer_comment_weeks),]

newcomers_week_data <- merge(newcomers_week_data,comment_week_summary,by=c("user_id","week"),all.x=TRUE)
newcomers_week_data <- merge(newcomers_week_data,cosine,by.x=c("user_id","user_week"),by.y=c("user_id","real_week"),all.x=TRUE)
newcomers_week_data <- newcomers_week_data[which(newcomers_week_data$user_id %in% newcomers_data$user_id),]
newcomers_week_data <- newcomers_week_data[which(newcomers_week_data$user_week<=12 & !is.na(newcomers_week_data$comment_no)),]

newcomers_data <- merge(newcomers_data,cosine[,c("user_id","domain_cosine","science_cosine","comment_week")],
                                  by.x=c("user_id","newcomer_comment_weeks"),by.y = c("user_id","comment_week"))

newcomers_data$Class <- as.factor(newcomers_data$retained) # 2610 

# remove those who would not have a chance to represent as a newcomer in our data
newcomers_data$newcomer_chance <- ifelse(newcomers_data$join_date <= add_with_rollback(as.Date("2022-03-14"), months(-3), roll_to_first = TRUE),1,0) 
#98 people did not have opportunity to become newcomer

newcomers_data <- newcomers_data[which(newcomers_data$newcomer_chance==1),]
newcomers_data <- newcomers_data[which(newcomers_data$since_start>=0),]

newcomers_week_data <- newcomers_week_data[which(newcomers_week_data$user_id %in% newcomers_data$user_id),]
```




```{r, eval=FALSE}



```

```{r}
# Comment data


newcomers_data <-  merge(newcomers_data,newcomer_threads_user, by=c("user_id"), all.x = TRUE)



```


```{r }
####### USER GROWTH
join_summary_week <- newcomers_data_org %>% 
   group_by(join_date) %>% 
   summarize(newcomers = length(user_id),
             newcomer_commenters = length(user_id[which(newcomer_comment >=1)]),
             percent_commenters = newcomer_commenters/newcomers)

join_summary_week_d <- newcomers_data_org %>% 
   group_by(join_date) %>% 
   summarize(newcomers = length(user_id),
             newcomer_commenters = length(user_id[which(newcomer_comment >=1)]),
             percent_commenters = newcomer_commenters/newcomers)


week_pct_comment <- ggplot(join_summary_week, aes(x=join_date, y=percent_commenters)) + 
  #geom_line(size=.7,color="#56B4E9") + 
  geom_smooth(size=.7,color="#56B4E9") +
  labs(x="Month",y="Language additions") + 
  scale_x_date(date_labels="%b %y",date_breaks  ="4 month") + 
  theme_classic() +  
  scale_y_continuous(labels = function(x) paste0(x, "%")) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1),
        legend.position="bottom", 
        legend.title = element_blank(),
        axis.text = element_text(size = 14, face="bold"),
        axis.title = element_text(size = 14, face="bold"))  
  geom_vline(aes(xintercept=c("2016-10-01")),linetype="dashed", color = "grey22",size=.7)

join_summary_month <- newcomers_data_org %>% 
   group_by(month=floor_date(join_date, "month")) %>% 
   summarize(newcomers = length(user_id),
             newcomer_commenters = length(user_id[which(newcomer_comment >=1)]),
             percent_commenters = newcomer_commenters/newcomers)

month_pct_comment <-  ggplot(join_summary_month, aes(x=month, y=percent_commenters)) + 
  #geom_line(size=.7,color="#56B4E9") + 
  geom_smooth(size=.7,color="#56B4E9") +
  labs(x="Month-Year",y="Newcomers commentings (%)") + 
  scale_x_date(date_labels="%b %y",date_breaks  ="4 month") + 
  theme_classic() +  
  scale_y_continuous(labels = function(x) paste0(x*100, "%")) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1),
        legend.position="bottom", 
        legend.title = element_blank(),
        axis.text = element_text(size = 14, face="bold"),
        axis.title = element_text(size = 14, face="bold"))  
  geom_vline(aes(xintercept=as.Date("2016-10-01")),linetype="dashed", color = "grey22",size=.7)
  

# pdf("/Users/coreyjackson/Library/CloudStorage/Box-Box/_research/language[collab]/community_lang_growth.pdf", width=5, height=5)
# community_lang_growth
# dev.off()
#   
# pdf("/Users/coreyjackson/Library/CloudStorage/Box-Box/_research/language[collab]/month_pct_comment.pdf", width=5, height=5)
# month_pct_comment
# dev.off()
# 
# pdf("/Users/coreyjackson/Library/CloudStorage/Box-Box/_research/language[collab]/week_pct_comment.pdf", width=5, height=5)
# week_pct_comment
# dev.off()
```


```{r}
## Cosine similarity 
newcomers_week_data$comment_week.y <- NULL
names(newcomers_week_data)[19] <- "comment_week"

newcomers_week_data$retained <- ifelse(newcomers_week_data$user_id %in% newcomers_data_org$user_id[which(newcomers_data_org$retained==1)],"Retained","Dropout")

####### VIZ BY THE WEEK THAT A NEWCOMER COMMENTED

domain_similarity.viz <- newcomers_week_data %>% 
group_by(comment_week) %>% 
summarise(domain_cosine = round(mean(domain_cosine, na.rm=TRUE),digits=2)) %>% 
  ggplot(aes(factor(comment_week), domain_cosine, label=domain_cosine)) +
  geom_col() + 
  labs(title="Domain similarity", 
         x="Newcomer week", y = "Domain similarity")+
   scale_fill_manual(values=c('black','lightgray'))+
   theme_classic() + 
    theme(
        axis.text = element_text(size = 14, face="bold"),
        axis.title = element_text(size = 14, face="bold"))  

field_similarity.viz <-  newcomers_week_data %>% 
group_by(comment_week) %>% 
summarise(science_cosine = round(mean(science_cosine, na.rm=TRUE),digits=2)) %>% 
  ggplot(aes(factor(comment_week), science_cosine, label=science_cosine)) +
  geom_col() + 
  labs("Science similarity",x="Newcomer week", y = "Science similarity")+
   scale_fill_manual(values=c('black','lightgray'))+
   theme_classic()+ 
    theme(
        axis.text = element_text(size = 14, face="bold"),
        axis.title = element_text(size = 14, face="bold"))  

domain_similarity_group.viz <-  newcomers_week_data %>% 
group_by(comment_week, retained) %>% 
summarise(domain_cosine = round(mean(domain_cosine, na.rm=TRUE),digits=2)) %>% 
  ggplot(aes(factor(comment_week), domain_cosine, label=domain_cosine, fill=as.factor(retained))) +
   geom_bar(stat="identity", position="dodge",preserve = "single")  +  
   geom_col(position = "dodge") +
  labs(title="Domain similarity", 
         x="Newcomer week", y = "Domain similarity", fill="Status")+
   scale_fill_manual(values=c('black','lightgray'))+
   theme_classic() + 
   theme(legend.position = c(0.2, 0.8),
         axis.text = element_text(size = 14, face="bold"),
        axis.title = element_text(size = 14, face="bold"))


field_similarity_group.viz <-  newcomers_week_data %>% 
group_by(comment_week, retained) %>% 
summarise(science_cosine = round(mean(science_cosine, na.rm=TRUE),digits=2)) %>% 
  ggplot(aes(factor(comment_week), science_cosine, label=science_cosine, fill=as.factor(retained))) +
   geom_bar(stat="identity", position="dodge",preserve = "single")  +  
   geom_col(position = "dodge") +
  labs(title="Science similarity", 
         x="Newcomer week", y = "Science similarity", fill="Status")+
   scale_fill_manual(values=c('black','lightgray'))+
   theme_classic() + 
     theme(legend.position = c(0.2, 0.8),
         axis.text = element_text(size = 14, face="bold"),
        axis.title = element_text(size = 14, face="bold"))

# pdf("/Users/jackson/Library/CloudStorage/Box-Box/_research/language[collab]/www22-figures/similarity_growth.pdf", width=11, height=4)
# #pdf("/Users/coreyjackson/Library/CloudStorage/Box-Box/_research/language[collab]/lang_growth.pdf", width=11, height=4)
# grid.arrange(domain_similarity.viz,field_similarity.viz, ncol = 2)
# dev.off()
# 
# pdf("/Users/jackson/Library/CloudStorage/Box-Box/_research/language[collab]/www22-figures/similarity_week_growth.pdf", width=11, height=4)
# #pdf("/Users/coreyjackson/Library/CloudStorage/Box-Box/_research/language[collab]/lang_growth.pdf", width=11, height=4)
# grid.arrange(domain_similarity_group.viz,field_similarity_group.viz, ncol = 2)
# dev.off()


####### VIZ BY THE NEWCOMERS TENURE WEEK (by comment week order)

domain_similarityUW.viz <- newcomers_week_data %>% 
group_by(user_week) %>% 
summarise(domain_cosine = round(mean(domain_cosine, na.rm=TRUE),digits=2)) %>% 
  ggplot(aes(factor(user_week), domain_cosine, label=domain_cosine)) +
  geom_col() + 
  labs(title="Domain similarity", 
         x="Commenting week", y = "Domain similarity")+
   scale_fill_manual(values=c('black','lightgray'))+
   theme_classic() + 
    theme(
        axis.text = element_text(size = 14, face="bold"),
        axis.title = element_text(size = 14, face="bold"))  

field_similarityUW.viz <-  newcomers_week_data %>% 
group_by(user_week) %>% 
summarise(science_cosine = round(mean(science_cosine, na.rm=TRUE),digits=2)) %>% 
  ggplot(aes(factor(user_week), science_cosine, label=science_cosine)) +
  geom_col() + 
  labs(title="Science similarity",x="Commenting week", y = "Science similarity")+
   scale_fill_manual(values=c('black','lightgray'))+
   theme_classic() + 
    theme(
         axis.text = element_text(size = 14, face="bold"),
        axis.title = element_text(size = 14, face="bold"))

domain_similarity_groupUW.viz <-  newcomers_week_data %>% 
group_by(user_week, retained) %>% 
summarise(domain_cosine = round(mean(domain_cosine, na.rm=TRUE),digits=2)) %>% 
  ggplot(aes(factor(user_week), domain_cosine, label=domain_cosine, fill=as.factor(retained))) +
   geom_bar(stat="identity", position="dodge",preserve = "single")  +  
   geom_col(position = "dodge") +
  labs(title="Domain similarity", 
         x="Commenting week", y = "Domain similarity", fill="Status")+
   scale_fill_manual(values=c('black','lightgray'))+
   theme_classic() + 
     theme(legend.position = c(0.2, 0.8),
         axis.text = element_text(size = 14, face="bold"),
        axis.title = element_text(size = 14, face="bold"))

field_similarity_groupUW.viz <-  newcomers_week_data %>% 
group_by(user_week, retained) %>% 
summarise(science_cosine = round(mean(science_cosine, na.rm=TRUE),digits=2)) %>% 
  ggplot(aes(factor(user_week), science_cosine, label=science_cosine, fill=as.factor(retained))) +
   geom_bar(stat="identity", position="dodge",preserve = "single")  +  
   geom_col(position = "dodge") +
  labs(title="Science similarity", 
         x="Commenting week", y = "Science similarity", fill="Status")+
   scale_fill_manual(values=c('black','lightgray'))+
   theme_classic() + 
     theme(legend.position = c(0.2, 0.8),
         axis.text = element_text(size = 14, face="bold"),
        axis.title = element_text(size = 14, face="bold"))

# pdf("/Users/jackson/Library/CloudStorage/Box-Box/_research/language[collab]/www22-figures/similarity_growthUW.pdf", width=11, height=4)
# #pdf("/Users/coreyjackson/Library/CloudStorage/Box-Box/_research/language[collab]/lang_growth.pdf", width=11, height=4)
# grid.arrange(domain_similarityUW.viz,field_similarityUW.viz, ncol = 2)
# dev.off()
# 
# pdf("/Users/jackson/Library/CloudStorage/Box-Box/_research/language[collab]/www22-figures/similarity_week_growthUW.pdf", width=11, height=4)
# #pdf("/Users/coreyjackson/Library/CloudStorage/Box-Box/_research/language[collab]/lang_growth.pdf", width=11, height=4)
# grid.arrange(domain_similarity_groupUW.viz,field_similarity_groupUW.viz, ncol = 2)
# dev.off()


####### VIZ BY GAINS NEWCOMERS TENURE (comment_week) WEEK (domain_change, science_change)

domain_change.viz <- newcomers_week_data %>% 
group_by(comment_week) %>% 
summarise(domain_change = round(mean(domain_change, na.rm=TRUE),digits=2)) %>% 
  ggplot(aes(factor(comment_week), domain_change, label=domain_change)) +
  geom_col() + 
  labs(title="Domain similarity change", 
         x="User week", y = "Domain similarity change")+
   scale_fill_manual(values=c('black','lightgray'))+
   theme_classic() + 
    theme(
        axis.text = element_text(size = 14, face="bold"),
        axis.title = element_text(size = 14, face="bold"))  

field_change.viz <-  newcomers_week_data %>% 
group_by(comment_week) %>% 
summarise(science_change = round(mean(science_change, na.rm=TRUE),digits=2)) %>% 
  ggplot(aes(factor(comment_week), science_change, label=science_change)) +
  geom_col() + 
  labs("Science similarity change",x="User week", y = "Science similarity change")+
   scale_fill_manual(values=c('black','lightgray'))+
   theme_classic() + 
    theme(
         axis.text = element_text(size = 14, face="bold"),
        axis.title = element_text(size = 14, face="bold"))

domain_change_group.viz <-  newcomers_week_data %>% 
group_by(comment_week, retained) %>% 
summarise(domain_change = round(mean(domain_change, na.rm=TRUE),digits=2)) %>% 
  ggplot(aes(factor(comment_week), domain_change, label=domain_change, fill=as.factor(retained))) +
   geom_bar(stat="identity", position="dodge",preserve = "single")  +  
   geom_col(position = "dodge") +
   ylim(0,0.05) +
  labs(title="Domain similarity change", 
         x="User week", y = "Domain similarity change", fill="Status")+
   scale_fill_manual(values=c('black','lightgray'))+
   theme_classic() + 
     theme(legend.position = c(0.2, 0.8),
         axis.text = element_text(size = 14, face="bold"),
        axis.title = element_text(size = 14, face="bold"))


field_change_group.viz <-  newcomers_week_data %>% 
group_by(comment_week, retained) %>% 
summarise(science_change = round(mean(science_change, na.rm=TRUE),digits=2)) %>% 
  ggplot(aes(factor(comment_week), science_change, label=science_change, fill=as.factor(retained))) +
   geom_bar(stat="identity", position="dodge",preserve = "single")  +  
  ylim(0,0.05) +
  geom_col(position = "dodge") +
  labs(title="Science similarity change", 
         x="User week", y = "Science similarity change", fill="Status")+
   scale_fill_manual(values=c('black','lightgray'))+
   theme_classic() + 
     theme(legend.position = c(0.5, 0.8),
         axis.text = element_text(size = 14, face="bold"),
        axis.title = element_text(size = 14, face="bold"))

# pdf("/Users/jackson/Library/CloudStorage/Box-Box/_research/language[collab]/www22-figures/similarity_change.pdf", width=11, height=4)
# #pdf("/Users/coreyjackson/Library/CloudStorage/Box-Box/_research/language[collab]/lang_growth.pdf", width=11, height=4)
# grid.arrange(domain_change.viz,field_change.viz, ncol = 2)
# dev.off()
# 
# pdf("/Users/jackson/Library/CloudStorage/Box-Box/_research/language[collab]/www22-figures/similarity_week_change.pdf", width=11, height=4)
# #pdf("/Users/coreyjackson/Library/CloudStorage/Box-Box/_research/language[collab]/lang_growth.pdf", width=11, height=4)
# grid.arrange(domain_change_group.viz,field_change_group.viz, ncol = 2)
# dev.off()

####### VIZ BY GAINS NEWCOMERS TENURE (by comment week order) WEEK (domain_change, science_change)

domain_changeUW.viz <- newcomers_week_data %>% 
group_by(user_week) %>% 
summarise(domain_change = round(mean(domain_change, na.rm=TRUE),digits=2)) %>% 
  ggplot(aes(factor(user_week), domain_change, label=domain_change)) +
  geom_col() + 
  labs(title="Domain similarity change", 
         x="Commenting week", y = "Domain similarity change")+
   scale_fill_manual(values=c('black','lightgray'))+
   theme_classic() + 
    theme(
        axis.text = element_text(size = 14, face="bold"),
        axis.title = element_text(size = 14, face="bold"))  

field_changeUW.viz <-  newcomers_week_data %>% 
group_by(user_week) %>% 
summarise(science_change = round(mean(science_change, na.rm=TRUE),digits=2)) %>% 
  ggplot(aes(factor(user_week), science_change, label=science_change)) +
  geom_col() + 
  labs("Science similarity change",x="Commenting week", y = "Science similarity change")+
   scale_fill_manual(values=c('black','lightgray'))+
   theme_classic() + 
    theme(
         axis.text = element_text(size = 14, face="bold"),
        axis.title = element_text(size = 14, face="bold"))

domain_change_groupUW.viz <-  newcomers_week_data %>% 
group_by(user_week, retained) %>% 
summarise(domain_change = round(mean(domain_change, na.rm=TRUE),digits=2)) %>% 
  ggplot(aes(factor(user_week), domain_change, label=domain_change, fill=as.factor(retained))) +
   geom_bar(stat="identity", position="dodge",preserve = "single")  +  
   geom_col(position = "dodge") +
  labs(title="Domain similarity change", 
         x="Commenting week", y = "Domain similarity change", fill="Status")+
   scale_fill_manual(values=c('black','lightgray'))+
   theme_classic() + 
     theme(legend.position = c(0.2, 0.8),
         axis.text = element_text(size = 14, face="bold"),
        axis.title = element_text(size = 14, face="bold"))


field_change_groupUW.viz <-  newcomers_week_data %>% 
group_by(user_week, retained) %>% 
summarise(science_change = round(mean(science_change, na.rm=TRUE),digits=2)) %>% 
  ggplot(aes(factor(user_week), science_change, label=science_change, fill=as.factor(retained))) +
   geom_bar(stat="identity", position="dodge",preserve = "single")  +  
   geom_col(position = "dodge") +
  labs(title="Science similarity change", 
         x="Commenting week", y = "Science similarity change", fill="Status")+
   scale_fill_manual(values=c('black','lightgray'))+
   theme_classic() + 
     theme(legend.position = c(0.2, 0.8),
         axis.text = element_text(size = 14, face="bold"),
        axis.title = element_text(size = 14, face="bold"))

# pdf("/Users/jackson/Library/CloudStorage/Box-Box/_research/language[collab]/www22-figures/similarity_changeUW.pdf", width=11, height=4)
# #pdf("/Users/coreyjackson/Library/CloudStorage/Box-Box/_research/language[collab]/lang_growth.pdf", width=11, height=4)
# grid.arrange(domain_changeUW.viz,field_changeUW.viz, ncol = 2)
# dev.off()
# 
# pdf("/Users/jackson/Library/CloudStorage/Box-Box/_research/language[collab]/www22-figures/similarity_week_changeUW.pdf", width=11, height=4)
# #pdf("/Users/coreyjackson/Library/CloudStorage/Box-Box/_research/language[collab]/lang_growth.pdf", width=11, height=4)
# grid.arrange(domain_change_groupUW.viz,field_change_groupUW.viz, ncol = 2)
# dev.off()
```


```{r, commentersmodels, echo=FALSE, include=FALSE, warning=FALSE}
################################
#   COMMENTERS MODELS          #
################################

##### Retained 
newcomer_info_commenters <- newcomers_data[which(newcomers_data$newcomer_comment>=1),]
newcomer_info_commenters <- newcomer_info_commenters[which(newcomer_info_commenters$newcomer_weeks >= 2 & newcomer_info_commenters$newcomer_comment>=1),] #1407

newcomer_info_commenters <- newcomer_info_commenters[,c(1:43,45,46,44)]

newcomer_info_commenters.show <- newcomer_info_commenters[,c(1,3,4,7,11,12,21,27,33:38,41,42,43,45,46)]
names(newcomer_info_commenters.show)[1:19] <- c("User","Classifications","Comments","Sessions","First_Comment_Sequence",
                                                "First_Comment_Session","Domain_Slope","Field_Slope","Questions","Replies", 
                                                "URLs","Tokens","Tags", "User_references","Days_Since_Launch","Domain_Cosine",
                                                "Field_Cosine","Threads_created","Class")

trainIndex_commenter <- createDataPartition(newcomer_info_commenters.show$Class, p = .7,
                                  list = FALSE,
                                  times = 1)
train_commenter <- newcomer_info_commenters.show[ trainIndex_commenter,]
valid_commenter <- newcomer_info_commenters.show[-trainIndex_commenter,]


retention_commenters.glm <- glm(Class ~ Classifications+Comments+Sessions+Questions+URLs+Tokens+Tags+User_references+Threads_created+Days_Since_Launch+First_Comment_Sequence+First_Comment_Session+Domain_Slope+Domain_Cosine+Field_Slope+Field_Cosine,data = train_commenter,family="binomial")

 logit2prob(coef(retention_commenters.glm)) #coefficients
 nagelkerke(retention_commenters.glm) # performance measures
 
 predictcommenter <- predict(retention_commenters.glm , newdata = valid_commenter, type = 'response')
 predictcommenter <- as.integer(predictcommenter>0.5)
 confusionMatrix(as.factor(predictcommenter),valid_commenter$Class)


########## PREPARING WEEKLY MODEL DATA 
week_model <- newcomers_week_data[which(!is.na(newcomers_week_data$variable)),]
week_model$week_class <- log(week_model$week_class+1)
week_model$comment_week <- as.factor(week_model$comment_week)

week_model.show <- week_model[,c(1,5:7,11:16,21,23,24,27)]
names(week_model.show)[1:14] <- c("User","Classifications","Comments","Sessions","Questions","Replies","URLs","Tokens","Tags",
                                  "User_references","Domain_Cosine","Last_session","Field_Cosine","Threads_created")
# Science cosine and change https://crumplab.com/psyc7709_2019/book/docs/a-tutorial-for-using-the-lme-function-from-the-nlme-package-.html 
science.lme <- lme(Field_Cosine ~ Classifications + Comments + Sessions + Questions + URLs + Tokens + Tags + User_references + Threads_created + Last_session, ~ 1 | User , data = week_model.show)
 nagelkerke(science.lme) # performance measures

##### Domain similarity score
domain.lme <- lme(Domain_Cosine ~ Classifications + Comments + Sessions + Questions + URLs + Tokens + Tags + User_references + Threads_created + Last_session, ~ 1 | User , data = week_model.show)
 nagelkerke(domain.lme) # performance measures
 

# https://easystats.github.io/performance/ 


```

## Retention models
```{r mixedmodel2, results = "asis", echo=TRUE, include=TRUE}
stargazer(retention_commenters.glm, science.lme,domain.lme, title="Retention Models",star.cutoffs = c(0.05, 0.01, 0.001), dep.var.labels=c("Retained (=1)"),
          object.names = TRUE,
          font.size = "small",
          align = TRUE,
          #omit.stat=c("f", "ser"),
          column.sep.width = "-10pt",
          no.space = TRUE, # to remove the spaces after each line of coefficients
          type = "latex",
          t.auto=F, p.auto=F, 
          #ci = TRUE, 
          report = ('vcs*'), 
          single.row = TRUE,
          digits=2
          
) #covariate.labels=c("Agent Centered","Authority Subject","Communal")
```


```{r, eval=FALSE}

examples <- c("513912","1412875","315840","2538","1692922","2066383")
newcomers_case <- newcomers_data[which(newcomers_data$user_id %in% examples),]
comments_case <- comments[which(comments$comment_user_id %in% examples),]

## Count science/domain words in comments 
comments_case$domain_words <- stringr::str_count(comments_case$comment_body, paste(domain, collapse='|'))
comments_case$field_words <- stringr::str_count(comments_case$comment_body, paste(field, collapse='|'))

write.csv(newcomers_case,"/Volumes/cbjackson2/language models/narrativecase/newcomers_case.csv")
write.csv(comments_case,"/Volumes/cbjackson2/language models/narrativecase/comments_case.csv")

```

